{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f219a2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\plantdisease\" (use force=True to force download)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets --quiet\n",
    "import opendatasets as od\n",
    "od.download('https://www.kaggle.com/datasets/emmarex/plantdisease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9eea67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19824c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b63f0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 20638\n"
     ]
    }
   ],
   "source": [
    "root_path = 'plantdisease/PlantVillage/'\n",
    "img_path = []\n",
    "labels_path = []\n",
    "\n",
    "for label in os.listdir(root_path):\n",
    "    for item in os.listdir(f'{root_path}/{label}'):\n",
    "        img_path.append(f'{root_path}/{label}/{item}')\n",
    "        labels_path.append(label)\n",
    "        \n",
    "print(f'Number of Images: {len(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0abad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Tomato__Tomato_YellowLeaf__Curl_Virus          3208\n",
      "Tomato_Bacterial_spot                          2127\n",
      "Tomato_Late_blight                             1909\n",
      "Tomato_Septoria_leaf_spot                      1771\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite    1676\n",
      "Tomato_healthy                                 1591\n",
      "Pepper__bell___healthy                         1478\n",
      "Tomato__Target_Spot                            1404\n",
      "Potato___Early_blight                          1000\n",
      "Potato___Late_blight                           1000\n",
      "Tomato_Early_blight                            1000\n",
      "Pepper__bell___Bacterial_spot                   997\n",
      "Tomato_Leaf_Mold                                952\n",
      "Tomato__Tomato_mosaic_virus                     373\n",
      "Potato___healthy                                152\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plantdisease/PlantVillage//Pepper__bell___Bact...</td>\n",
       "      <td>Pepper__bell___Bacterial_spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plantdisease/PlantVillage//Pepper__bell___Bact...</td>\n",
       "      <td>Pepper__bell___Bacterial_spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plantdisease/PlantVillage//Pepper__bell___Bact...</td>\n",
       "      <td>Pepper__bell___Bacterial_spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plantdisease/PlantVillage//Pepper__bell___Bact...</td>\n",
       "      <td>Pepper__bell___Bacterial_spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plantdisease/PlantVillage//Pepper__bell___Bact...</td>\n",
       "      <td>Pepper__bell___Bacterial_spot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path   \n",
       "0  plantdisease/PlantVillage//Pepper__bell___Bact...  \\\n",
       "1  plantdisease/PlantVillage//Pepper__bell___Bact...   \n",
       "2  plantdisease/PlantVillage//Pepper__bell___Bact...   \n",
       "3  plantdisease/PlantVillage//Pepper__bell___Bact...   \n",
       "4  plantdisease/PlantVillage//Pepper__bell___Bact...   \n",
       "\n",
       "                           label  \n",
       "0  Pepper__bell___Bacterial_spot  \n",
       "1  Pepper__bell___Bacterial_spot  \n",
       "2  Pepper__bell___Bacterial_spot  \n",
       "3  Pepper__bell___Bacterial_spot  \n",
       "4  Pepper__bell___Bacterial_spot  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of our Dataframe\n",
    "data_df = pd.DataFrame(zip(img_path, labels_path), columns = ['image_path', 'label'])\n",
    "\n",
    "# Print the distribution of data among classes and the format of our DataFrame.\n",
    "print(data_df['label'].value_counts())\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4cd21e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 16510, Validation size: 2064, Test size: 2064\n"
     ]
    }
   ],
   "source": [
    "train = data_df.sample(frac=0.8)\n",
    "val = data_df.drop(train.index)\n",
    "test = val.sample(frac=0.5)\n",
    "val = val.drop(test.index)\n",
    "\n",
    "print(f'Train size: {len(train)}, Validation size: {len(val)}, Test size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78d692c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelEncoder for the Labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data_df['label'])\n",
    "\n",
    "# Create a transform for transforming the images in the same - appropriate form\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(dtype=torch.float)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88e8a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantsDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.labels = torch.tensor(label_encoder.transform(dataframe['label']), dtype=torch.long).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        image = Image.open(self.dataframe.iloc[indx, 0]).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image).to(device)\n",
    "        \n",
    "        label = self.labels[indx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e17d6971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.6549, 0.6745, 0.6863,  ..., 0.3961, 0.5020, 0.5098],\n",
      "         [0.7137, 0.6980, 0.6353,  ..., 0.5176, 0.5451, 0.4745],\n",
      "         [0.6275, 0.7059, 0.6471,  ..., 0.4980, 0.5490, 0.5137],\n",
      "         ...,\n",
      "         [0.7373, 0.6275, 0.5490,  ..., 0.5608, 0.6118, 0.5686],\n",
      "         [0.6980, 0.6941, 0.6745,  ..., 0.5569, 0.6078, 0.5176],\n",
      "         [0.6510, 0.6588, 0.7216,  ..., 0.5294, 0.5647, 0.6196]],\n",
      "\n",
      "        [[0.6078, 0.6275, 0.6392,  ..., 0.3373, 0.4431, 0.4510],\n",
      "         [0.6667, 0.6510, 0.5882,  ..., 0.4588, 0.4863, 0.4157],\n",
      "         [0.5804, 0.6588, 0.6000,  ..., 0.4392, 0.4902, 0.4549],\n",
      "         ...,\n",
      "         [0.6902, 0.5804, 0.5020,  ..., 0.4980, 0.5490, 0.5059],\n",
      "         [0.6510, 0.6471, 0.6275,  ..., 0.4941, 0.5451, 0.4549],\n",
      "         [0.6039, 0.6118, 0.6745,  ..., 0.4667, 0.5020, 0.5569]],\n",
      "\n",
      "        [[0.6235, 0.6431, 0.6549,  ..., 0.3569, 0.4627, 0.4706],\n",
      "         [0.6824, 0.6667, 0.6039,  ..., 0.4784, 0.5059, 0.4353],\n",
      "         [0.5961, 0.6745, 0.6157,  ..., 0.4588, 0.5098, 0.4745],\n",
      "         ...,\n",
      "         [0.7059, 0.5961, 0.5176,  ..., 0.5098, 0.5608, 0.5176],\n",
      "         [0.6667, 0.6627, 0.6431,  ..., 0.5059, 0.5569, 0.4667],\n",
      "         [0.6196, 0.6275, 0.6902,  ..., 0.4784, 0.5137, 0.5686]]]), tensor(10))\n"
     ]
    }
   ],
   "source": [
    "train_data = PlantsDataset(train, transform=transform)\n",
    "val_data = PlantsDataset(val, transform=transform)\n",
    "test_data = PlantsDataset(test, transform=transform)\n",
    "\n",
    "print(val_data.__getitem__(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82fc1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 1e-3\n",
    "Batch_Size = 16\n",
    "Epochs = 5\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=Batch_Size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=Batch_Size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e2fc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class Plants(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv2d1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2d2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv2d3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv2d4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Maxpooling\n",
    "        self.maxpooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Activation Function\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Dense Layers\n",
    "        self.dense1 = nn.Linear((128*16*16), 256)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.dense3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, number_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "                                    # x = (3, 256, 256)\n",
    "        # Stage 1\n",
    "        x = self.conv2d1(x)        # (16, 256, 256)\n",
    "        x = self.maxpooling(x)     # (16, 128, 128)\n",
    "        x = self.activation(x)     # (16, 128, 128)\n",
    "        \n",
    "        # Stage 2\n",
    "        x = self.conv2d2(x)        # (32, 128, 128)\n",
    "        x = self.maxpooling(x)     # (32, 64, 64)\n",
    "        x = self.activation(x)     # (32, 64, 64)\n",
    "    \n",
    "        # Stage 3\n",
    "        x = self.conv2d3(x)        # (64, 64, 64)\n",
    "        x = self.maxpooling(x)     # (64, 32, 32)\n",
    "        x = self.activation(x)     # (64, 32, 32)\n",
    "\n",
    "        # Stage 4\n",
    "        x = self.conv2d4(x)        # (128, 32, 32)\n",
    "        x = self.maxpooling(x)     # (128, 16, 16)\n",
    "        x = self.activation(x)     # (128, 16, 16)\n",
    "\n",
    "        # Stage 5\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Stage 6\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Stage 7\n",
    "        x = self.dense2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Stage 8\n",
    "        x = self.dense3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Stage 9\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Plants_Small(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv2d1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2d2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Maxpooling\n",
    "        self.maxpooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Activation Function\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Dense Layers\n",
    "        self.dense = nn.Linear((32*64*64), 32)\n",
    "        self.output = nn.Linear(32, number_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "                                    # x = (3, 256, 256)\n",
    "        # Stage 1\n",
    "        x = self.conv2d1(x)        # (16, 256, 256)\n",
    "        x = self.maxpooling(x)     # (16, 128, 128)\n",
    "        x = self.activation(x)     # (16, 128, 128)\n",
    "        \n",
    "        # Stage 2\n",
    "        x = self.conv2d2(x)        # (32, 128, 128)\n",
    "        x = self.maxpooling(x)     # (32, 64, 64)\n",
    "        x = self.activation(x)     # (32, 64, 64)\n",
    "\n",
    "        # Stage 5\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Stage 6\n",
    "        x = self.dense(x)        \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "517d5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =  Plants(len(data_df['label'].unique())).to(device)\n",
    "model =  Plants_Small(len(data_df['label'].unique())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad88c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 256, 256]             448\n",
      "         MaxPool2d-2         [-1, 16, 128, 128]               0\n",
      "         LeakyReLU-3         [-1, 16, 128, 128]               0\n",
      "            Conv2d-4         [-1, 32, 128, 128]           4,640\n",
      "         MaxPool2d-5           [-1, 32, 64, 64]               0\n",
      "         LeakyReLU-6           [-1, 32, 64, 64]               0\n",
      "           Flatten-7               [-1, 131072]               0\n",
      "            Linear-8                   [-1, 32]       4,194,336\n",
      "            Linear-9                   [-1, 15]             495\n",
      "================================================================\n",
      "Total params: 4,199,919\n",
      "Trainable params: 4,199,919\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 19.00\n",
      "Params size (MB): 16.02\n",
      "Estimated Total Size (MB): 35.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e541d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=Learning_Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38fd1406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1032/1032 [04:23<00:00,  3.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:18<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Time: 4.7 min, Train Loss: 1.072 Train Accuracy: 67.7953\n",
      "              Validation Loss: 0.078 Validation Accuracy 79.312\n",
      "              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1032/1032 [04:07<00:00,  4.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:16<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Time: 4.41 min, Train Loss: 0.5113 Train Accuracy: 84.0097\n",
      "              Validation Loss: 0.0676 Validation Accuracy 81.3469\n",
      "              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1032/1032 [04:08<00:00,  4.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:17<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 Time: 4.43 min, Train Loss: 0.2915 Train Accuracy: 90.7813\n",
      "              Validation Loss: 0.0532 Validation Accuracy 86.3857\n",
      "              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1032/1032 [04:10<00:00,  4.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:17<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Time: 4.46 min, Train Loss: 0.1721 Train Accuracy: 94.5851\n",
      "              Validation Loss: 0.0781 Validation Accuracy 82.5097\n",
      "              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1032/1032 [04:13<00:00,  4.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:17<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 Time: 4.52 min, Train Loss: 0.1102 Train Accuracy: 96.3961\n",
      "              Validation Loss: 0.0915 Validation Accuracy 80.6686\n",
      "              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_loss_train_plot = []\n",
    "total_loss_validation_plot = []\n",
    "total_acc_train_plot = []\n",
    "total_acc_validation_plot = []\n",
    "time_plot = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    epoch_start = time.time()\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    total_loss_val = 0\n",
    "    total_acc_val = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        train_loss = criterion(outputs, labels)\n",
    "        total_loss_train += train_loss.item()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        train_acc = (torch.argmax(outputs, axis=1) == labels).sum().item()\n",
    "        \n",
    "        total_acc_train += train_acc\n",
    "        optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            total_loss_val += val_loss.item()\n",
    "            \n",
    "            val_acc = (torch.argmax(outputs, axis=1) == labels).sum().item()\n",
    "            total_acc_val += val_acc\n",
    "            \n",
    "    total_loss_train_plot.append(round(total_loss_train/1000, 4))\n",
    "    total_loss_validation_plot.append(round(total_loss_val/1000, 4))\n",
    "    \n",
    "    total_acc_train_plot.append(round((total_acc_train/train_data.__len__()) * 100, 4))\n",
    "    total_acc_validation_plot.append(round((total_acc_val/val_data.__len__()) * 100, 4))\n",
    "      \n",
    "    epoch_finish = round((time.time() - epoch_start)/60, 2)\n",
    "        \n",
    "    print(f'''Epoch {epoch+1}/{Epochs} Time: {epoch_finish} min, Train Loss: {round(total_loss_train/1000, 4)} Train Accuracy: {round((total_acc_train/train_data.__len__()) * 100, 4)}\n",
    "              Validation Loss: {round(total_loss_val/1000, 4)} Validation Accuracy {round((total_acc_val/val_data.__len__()) * 100, 4)}\n",
    "              ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "469815c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"plants_model_small.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67e797c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 256, 256]             448\n",
      "         MaxPool2d-2         [-1, 16, 128, 128]               0\n",
      "         LeakyReLU-3         [-1, 16, 128, 128]               0\n",
      "            Conv2d-4         [-1, 32, 128, 128]           4,640\n",
      "         MaxPool2d-5           [-1, 32, 64, 64]               0\n",
      "         LeakyReLU-6           [-1, 32, 64, 64]               0\n",
      "           Flatten-7               [-1, 131072]               0\n",
      "            Linear-8                   [-1, 32]       4,194,336\n",
      "            Linear-9                   [-1, 15]             495\n",
      "================================================================\n",
      "Total params: 4,199,919\n",
      "Trainable params: 4,199,919\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 19.00\n",
      "Params size (MB): 16.02\n",
      "Estimated Total Size (MB): 35.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model =  Plants_Small(len(data_df['label'].unique())).to(device)\n",
    "model.load_state_dict(torch.load(\"plants_model_small.pt\"))\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0a5b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▏                                                                  | 160/1032 [00:23<02:07,  6.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     13\u001b[0m         start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     14\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[45], line 14\u001b[0m, in \u001b[0;36mPlantsDataset.__getitem__\u001b[1;34m(self, indx)\u001b[0m\n\u001b[0;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[indx, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 14\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[indx]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation of the trained model\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "total_loss_test = 0\n",
    "total_acc_test = 0\n",
    "inference_time = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        \n",
    "        start = time.time()\n",
    "        outputs = model(images)\n",
    "        inference_t = round((time.time() - start), 2)\n",
    "        inference_time.append(inference_t)\n",
    "        \n",
    "        test_loss = criterion(outputs, labels)\n",
    "        total_loss_test += test_loss.item()\n",
    "\n",
    "        test_acc = (torch.argmax(outputs, axis=1) == labels).sum().item()\n",
    "        total_acc_test += test_acc\n",
    "        \n",
    "print(f\"Test Loss: {round(total_loss_test/1000, 4)}, Test Accuracy: {round(total_acc_test/test_data.__len__()*100, 4)}, Inference Time: {round(sum(inference_time)/len(inference_time), 4)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ed81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization and pruning of the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a72e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to tflite mode of the trained model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
